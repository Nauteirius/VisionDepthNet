{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VishionDepthNet\n",
        "\n",
        "Sample implementation of distance estimation in dashcam-like use cases.\n",
        "\n",
        "## Environment setup\n",
        "\n",
        "Install required libraries"
      ],
      "metadata": {
        "id": "FkxVDg-eBt-m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHPAJTmEuvrN"
      },
      "outputs": [],
      "source": [
        "!pip install einops==0.8.1\n",
        "!pip install matplotlib==3.10.0 matplotlib-inline==0.1.7 matplotlib-venn==1.1.2\n",
        "!pip install numpy==2.0.2\n",
        "!pip install opencv-contrib-python==4.11.0.86 opencv-python==4.11.0.86 opencv-python-headless==4.11.0.86\n",
        "!pip install timm==1.0.15\n",
        "!pip install ultralytics==8.3.102 ultralytics-thop==2.0.14\n",
        "\n",
        "# Install PyTorch (compatible version for Colab, CPU/GPU auto)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depth estimation\n",
        "\n",
        "In this code block, we demonstrate how to utilize YOLO and MiDaS models for distance estimation. First, we define YOLO classes to identify (cars, bikes and pedestrians). Once the bounding boxes are detected, we apply MiDaS depth estimation map on the image that gives relative (!) distance distribution from the camera perspective. As one-shot distance estimation is generally a difficult task even for humans, we experimentally develop a formula to convert the depths to meters - feel free to play around with both the scaling factor and the formula itself."
      ],
      "metadata": {
        "id": "Z2KozIe_CNrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Calibration parameters\n",
        "SCALE_FACTOR = 600_000\n",
        "CLASSES = [0, 1, 2]     # 0 = person, 1 = bike, 2 = car\n",
        "FRAME_SIZE = (384, 640)\n",
        "\n",
        "# 2. Detection and segmentation function\n",
        "def detect_and_segment(image_input, conf_threshold=0.5):\n",
        "    model = YOLO(\"yolov8s-seg.pt\")  # Make sure this file is available\n",
        "    results = model.predict(image_input, conf=conf_threshold, classes=CLASSES)\n",
        "\n",
        "    detections = []\n",
        "    for result in results:\n",
        "        for i, box in enumerate(result.boxes):\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "            mask = result.masks[i].data[0].cpu().numpy()\n",
        "            mask = cv2.resize(mask, (result.orig_shape[1], result.orig_shape[0]))\n",
        "\n",
        "            detections.append({\n",
        "                \"bbox\": (x1, y1, x2, y2),\n",
        "                \"mask\": mask,\n",
        "                \"class\": result.names[int(box.cls[0])],\n",
        "                \"confidence\": box.conf[0].item()\n",
        "            })\n",
        "\n",
        "    return detections, results[0].orig_img\n",
        "\n",
        "# 3. Depth model initialization\n",
        "def setup_depth_estimator():\n",
        "    model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\", trust_repo=True)\n",
        "    model.eval()\n",
        "\n",
        "    transforms = Compose([\n",
        "        Resize(FRAME_SIZE),\n",
        "        ToTensor(),\n",
        "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    return model, transforms\n",
        "\n",
        "# 4. Depth estimation\n",
        "def estimate_depth(image, model, transforms):\n",
        "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    img_pil = Image.fromarray(img_rgb)\n",
        "    input_tensor = transforms(img_pil).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        disparity = model(input_tensor)\n",
        "\n",
        "    depth_map = disparity.squeeze().cpu().numpy()\n",
        "    return depth_map\n",
        "\n",
        "# 5. Distance calculation\n",
        "def calculate_distances(detections, depth_map, original_image_shape):\n",
        "    depth_map = cv2.resize(depth_map, (original_image_shape[1], original_image_shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    for det in detections:\n",
        "        mask = det[\"mask\"] > 0.5\n",
        "        object_depths = depth_map[mask]\n",
        "\n",
        "        if object_depths.size == 0:\n",
        "            det[\"distance\"] = None\n",
        "            continue\n",
        "\n",
        "        max_depth = np.max(object_depths)\n",
        "        average_depth = np.mean(object_depths)\n",
        "        det[\"distance\"] = average_depth\n",
        "\n",
        "        y, x = np.where((depth_map == max_depth) & mask)\n",
        "        det[\"closest_point\"] = (x[0], y[0]) if len(x) > 0 else None\n",
        "\n",
        "    return detections\n",
        "\n",
        "# 5.1 Convert depth map to meters\n",
        "def convert_to_meters(depth_map):\n",
        "    depth_map_meters = SCALE_FACTOR / (depth_map * depth_map)\n",
        "    return depth_map_meters\n",
        "\n",
        "# 6. Visualization\n",
        "def visualize(image, detections, depth_map, save_path=None):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "    display_image = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    for det in detections:\n",
        "        if det[\"distance\"] is None:\n",
        "            continue\n",
        "\n",
        "        mask = det[\"mask\"] > 0.5\n",
        "        overlay = display_image.copy()\n",
        "        overlay[mask] = [0, 255, 255]\n",
        "        cv2.addWeighted(overlay, 0.5, display_image, 0.5, 0, display_image)\n",
        "\n",
        "        x1, y1, x2, y2 = det[\"bbox\"]\n",
        "        cv2.rectangle(display_image, (x1, y1), (x2, y2), (0,255,0), 2)\n",
        "        print(f\"{det['class']} {det['distance']:.1f}m\")\n",
        "        cv2.putText(display_image, f\"{det['class']} {det['distance']:.1f}m\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
        "\n",
        "        if det[\"closest_point\"]:\n",
        "            x, y = det[\"closest_point\"]\n",
        "            cv2.circle(display_image, (x, y), 8, (255,0,0), -1)\n",
        "\n",
        "    if save_path:\n",
        "        cv2.imwrite(save_path, cv2.cvtColor(display_image, cv2.COLOR_RGB2BGR))\n",
        "    else:\n",
        "        ax1.imshow(display_image)\n",
        "        ax1.set_title(\"Detections with masks and distances\")\n",
        "        ax2.imshow(depth_map, cmap=\"plasma\")\n",
        "        ax2.set_title(\"Depth map\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "Jzcx_yehu0N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample usage of depth estimation\n",
        "\n",
        "We extracted subsequent parts of code into functions so that the flow is clear. Below we present how to use it step by step."
      ],
      "metadata": {
        "id": "3g2ON9NuDHkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "# Upload image if needed\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# Make sure you upload yolov8s-seg.pt model too\n",
        "# Or you can download it directly inside Colab\n",
        "# Example:\n",
        "!wget https://github.com/ultralytics/assets/releases/download/v8.0.0/yolov8s-seg.pt\n",
        "\n",
        "# 1. Detection + Segmentation\n",
        "detections, image = detect_and_segment(\"testimage.jpg\")\n",
        "original_shape = image.shape[:2]\n",
        "\n",
        "# 2. Depth model initialization\n",
        "depth_model, depth_transforms = setup_depth_estimator()\n",
        "\n",
        "# 3. Depth estimation\n",
        "depth_map = estimate_depth(image, depth_model, depth_transforms)\n",
        "depth_map_meters = convert_to_meters(depth_map)\n",
        "\n",
        "# 4. Distance calculations\n",
        "detections = calculate_distances(detections, depth_map_meters, original_shape)\n",
        "\n",
        "# 5. Visualization\n",
        "visualize(image, detections, depth_map)"
      ],
      "metadata": {
        "id": "McwxByMZu4cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Car copilot\n",
        "\n",
        "In the second part of the notebook we give a usage example to analyze frames of dashcam stream of the video in front of a moving car. Every frame is processed to detect pedestrians (example), find the closest one and estimate the velocity how quick the car approaches the pedestrian (based on FPS and distance difference between the two photos separated by number of frames, 30 in the code)."
      ],
      "metadata": {
        "id": "ItgOWDl5DgXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "FPS = 30\n",
        "FRAME_SIZE = (640, 448)\n",
        "\n",
        "def road_to_stop(velocity, deceleration=9.8, tire_friction=0.7):\n",
        "    effective_deceleration = deceleration * tire_friction\n",
        "    return (velocity ** 2) / (2 * effective_deceleration)\n",
        "\n",
        "def press_brake():\n",
        "    print(\"🚨 Pressing the brake! 🚨\")\n",
        "\n",
        "def process_image(frame):\n",
        "    frame = cv2.resize(frame, FRAME_SIZE)\n",
        "    detections, image = detect_and_segment(frame)\n",
        "    original_shape = image.shape[:2]\n",
        "\n",
        "    depth_model, depth_transforms = setup_depth_estimator()\n",
        "\n",
        "    depth_map = estimate_depth(image, depth_model, depth_transforms)\n",
        "    depth_map_meters = convert_to_meters(depth_map)\n",
        "\n",
        "    detections = calculate_distances(detections, depth_map_meters, original_shape)\n",
        "\n",
        "    visualize(image, detections, depth_map_meters, save_path=None)  # Show image instead of saving\n",
        "\n",
        "    min_distance = float('inf')\n",
        "\n",
        "    for det in detections:\n",
        "        if det[\"class\"] == \"person\" and det[\"distance\"] is not None:\n",
        "            print(f\"Detected {det['class']} at distance: {det['distance']:.2f} m\")\n",
        "            min_distance = min(det[\"distance\"], min_distance)\n",
        "\n",
        "    if min_distance == float('inf'):\n",
        "        print(\"INFO: No person detected.\")\n",
        "\n",
        "    return min_distance\n",
        "\n",
        "def car_copilot(dashcam_video_stream):\n",
        "    context = 50\n",
        "    distances = [0.0] * context\n",
        "    current_index = 0\n",
        "    frame_count = 0\n",
        "\n",
        "    # Initialize once!\n",
        "    depth_model, depth_transforms = setup_depth_estimator()\n",
        "\n",
        "    cap = cv2.VideoCapture(dashcam_video_stream)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Unable to open video file: {dashcam_video_stream}\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, FRAME_SIZE)\n",
        "\n",
        "        # Important: Now pass the preloaded depth_model to avoid reloading every frame\n",
        "        detections, image = detect_and_segment(frame)\n",
        "        original_shape = image.shape[:2]\n",
        "        depth_map = estimate_depth(image, depth_model, depth_transforms)\n",
        "        depth_map_meters = convert_to_meters(depth_map)\n",
        "        detections = calculate_distances(detections, depth_map_meters, original_shape)\n",
        "\n",
        "        min_distance = float('inf')\n",
        "        for det in detections:\n",
        "            if det[\"class\"] == \"person\" and det[\"distance\"] is not None:\n",
        "                min_distance = min(det[\"distance\"], min_distance)\n",
        "\n",
        "        if min_distance == float('inf'):\n",
        "            current_distance = 9999  # No person detected, set far distance\n",
        "        else:\n",
        "            current_distance = min_distance\n",
        "\n",
        "        if current_index < context:\n",
        "            distances[current_index] = current_distance\n",
        "        else:\n",
        "            previous_distance = distances[current_index % context]\n",
        "            delta_distance = current_distance - previous_distance\n",
        "            velocity = delta_distance * FPS\n",
        "\n",
        "            road_to_stop_distance = road_to_stop(velocity)\n",
        "            print(f\"Velocity: {velocity:.2f} m/s, Road distance to stop: {road_to_stop_distance:.2f} m\")\n",
        "\n",
        "            if road_to_stop_distance > current_distance:\n",
        "                print(\"⚠️ Warning: Car is NOT in safe stopping distance!\")\n",
        "                press_brake()\n",
        "\n",
        "        distances[current_index % context] = current_distance\n",
        "        current_index += 1\n",
        "        frame_count += 1\n",
        "\n",
        "        print(f\"Frame: {frame_count}, Distance to closest person: {current_distance:.2f} m\")\n",
        "\n",
        "    cap.release()\n",
        "    return frame_count"
      ],
      "metadata": {
        "id": "cJzmIRV2v6R3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample usage\n",
        "\n",
        "As before, you are free to play with the parameters or provide your own video. The usage of car copilot is easy from this point - just run the code block below."
      ],
      "metadata": {
        "id": "idIpBj7PEP4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the dashcam video\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# Now, run the co-pilot!\n",
        "car_copilot(\"testvideo.mp4\")  # replace filename if needed\n"
      ],
      "metadata": {
        "id": "RiJinGT9v-U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big thanks for the attention of yours, enjoy your time"
      ],
      "metadata": {
        "id": "JatceywsEnNq"
      }
    }
  ]
}